import dataclasses
from torch.fx import Node
import torch
from typing import Optional
from torch.utils._ordered_set import OrderedSet
import functools

from .collector import get_args_of_node_type, CantChunk, eligible_source_node_op_to_idx, get_fake_tensor_from_node, compute_tensor_size

aten = torch.ops.aten
prims = torch.ops.prims

# Rules to propagate chunking metadata from inputs to the current node
propagate_rules = {
}

def _register_propagate_rule(aten_op, handler):
    if not isinstance(aten_op, (list, tuple)):
        aten_op = [aten_op]

    for op in aten_op:
        propagate_rules[op] = handler
    return handler

def register_propagate_rule(aten_op):
    return functools.partial(_register_propagate_rule, aten_op)

@dataclasses.dataclass
class ChunkingMeta:
    # The value of the node should be scaled by the specified scalar
    # tensor. Need this sine we pretend tangent to be 1 first and
    # scale the affected tensor later. Need propagate such information
    # to downstream.
    scale_by: Node = None 

    # The dimension of the current tensor that get chunked.
    # Can be None if the current tensor is not chunked. E.g., when
    # the current tensor is a scalar tensor generated by summing a chunked tensor.
    #
    # To recover a tensor with non-None chunk_dim, we need concat each
    # chunk at the 'chunk_dim' dimension.
    chunk_dim: Optional[int] = None

    # The original tensor is the sum of each tensor in the chunk subgraph.
    # chunk_dim should be None if need_sum is True
    need_sum: bool = False

def format_node_with_chunking_meta(node: torch.fx.Node):
    from torch._inductor.runtime.runtime_utils import green_text
    fake_tensor = get_fake_tensor_from_node(node)
    shape = list(fake_tensor.shape) if fake_tensor is not None else "?"
    print(f"  {shape} {node.format_node()}")

    if meta := get_chunking_meta(node):
        print(f"    {green_text(str(meta))}")

def set_chunking_meta(node, meta=None, **kwargs):
    if meta is None:
        meta = ChunkingMeta(**kwargs)
    node.meta["chunking"] = meta
    format_node_with_chunking_meta(node)

def copy_chunking_meta(dst_node, src_node):
    if isinstance(src_node, torch.fx.Node):
        src_meta = get_chunking_meta(src_node)
    else:
        assert isinstance(src_node, ChunkingMeta)
        src_meta = src_node
    assert src_meta
    set_chunking_meta(dst_node, **src_meta.__dict__)

def get_chunking_meta(node):
    return node.meta.get("chunking")

class Propagator:
    @classmethod
    def find_nodes_to_recover(cls, chunking_subgraph_nodes: OrderedSet[Node]):
        """
        Nodes in chunking_subgraph_nodes that has external usage need
        to be recovered in the end.
        """
        to_recover = OrderedSet()
        for node in chunking_subgraph_nodes:
            if any(user not in chunking_subgraph_nodes for user in node.users):
                to_recover.add(node)
        return to_recover

    @classmethod
    def chunk_external_nodes(cls, graph, chunking_subgraph_nodes: OrderedSet[Node]):
        """
        Find all nodes that are suppose to be input to the chunking
        subgraph. Add chunking metadata to them..
        """

        source_user = next(node for node in chunking_subgraph_nodes if node.op != "placeholder")
        batch_size = source_user.meta["val"].size(0)
        assert source_user.target in eligible_source_node_op_to_idx

        # For source_user, only its input specificed by
        # eligible_source_node_op_to_idx[target] need to be chunked
        no_chunk_nodes = OrderedSet()
        for idx, node in enumerate(get_args_of_node_type(source_user)):
            if idx != eligible_source_node_op_to_idx[source_user.target]:
                no_chunk_nodes.add(node)

        external_args = OrderedSet()
        for node in chunking_subgraph_nodes:
            for idx, arg in enumerate(get_args_of_node_type(node)):
                if arg not in chunking_subgraph_nodes:
                    external_args.add(arg)

        with graph.inserting_before(source_user):
            for node in external_args:
                def _should_chunk(node):
                    if node.meta["val"].numel() == 1:
                        return False
                    
                    if node in no_chunk_nodes:
                        return False

                    # unwrap an permute
                    if node.target == aten.permute.default:
                        node = node.args[0]

                    if node in no_chunk_nodes:
                        return False

                    # Sanity check the tensor size
                    if node.meta["val"].size(0) != batch_size:
                        raise CantChunk("First dimension does not match batch_size. {batch_size=} v.s. {node.meta['val'].size(0)}.")
                    return True

                if _should_chunk(node):
                    # attach the chunking metadata
                    set_chunking_meta(node, chunk_dim=0)

    @classmethod
    def add_chunking_meta(cls, graph, chunking_subgraph_nodes: OrderedSet[Node]):
        # print(f"{cls.find_nodes_to_recover(chunking_subgraph_nodes)=}")
        cls.chunk_external_nodes(graph, chunking_subgraph_nodes)

        for node in chunking_subgraph_nodes:
            if node.op == "placeholder" and "tangent" in node.target:
                set_chunking_meta(node, scale_by=node)
                continue
        
            # weight of matmul is not chunked
            # assert all(get_chunking_meta(arg) is not None for arg in get_args_of_node_type(node)), f"{node.format_node()}"

            if node.op != "call_function":
                raise CantChunk("Chunker can only chunk call_function nodes")
            target = node.target
            if target not in propagate_rules:
                raise CantChunk(f"Missing propagation rule for target {target}: {node.format_node()}")

            if not propagate_rules[target](node):
                raise CantChunk(f"Propagate rule for {target} fail: {node.format_node()}")

# Begins propagation rules
@register_propagate_rule(aten.addmm.default)
def propagate_addmm(addmm_node):
    bias_node, input_node, weight_node = addmm_node.args

    # only input is chunked
    if get_chunking_meta(bias_node) is None and get_chunking_meta(weight_node) is None and get_chunking_meta(input_node) is not None:
        copy_chunking_meta(addmm_node, input_node)
        return True

    return False

@register_propagate_rule([
    prims.convert_element_type.default,
    aten.sub.Tensor,
    aten.exp.default,
    aten.log.default,
    aten.squeeze.dim,
    aten.gather.default,
    aten.neg.default,
])
def propagate_general_copy_from_input(out_node, allow_non_chunked_scalar_input=False):
    """
    This rule assumes
    1. The node has at least one Node input
    2. Each Node input should have chunking meta
    3. Different nodes have the same chunking meta

    Then just copy the chunking meta to the output
    """
    node_args = get_args_of_node_type(out_node)

    if allow_non_chunked_scalar_input:
        new_args = []
        for arg in node_args:
            if compute_tensor_size(arg, count_bytes=False) == 1:
                if get_chunking_meta(arg) is not None:
                    return False
            else:
                new_args.append(arg)
        node_args = new_args
    if len(node_args) == 0:
        return False
    
    src_meta = get_chunking_meta(node_args[0])
    if src_meta is None:
        return False

    for other_node in node_args[1:]:
        other_meta = get_chunking_meta(other_node)
        if other_meta != src_meta:
            return False

    copy_chunking_meta(out_node, src_meta)
    return True

@register_propagate_rule([
    aten.where.self,
])
def propagate_where(where_node):
    # where_node can have a non-chunked scalar input
    return propagate_general_copy_from_input(where_node, True)

@register_propagate_rule(aten.div.Tensor)
def propagate_div(div_node):
    lhs_node, rhs_node = div_node.args[:2]
    lhs_meta = get_chunking_meta(lhs_node)
    rhs_meta = get_chunking_meta(rhs_node)

    # Divide by a non-chunked scalar, just copy the metadata from
    # the numerator.
    if lhs_meta is not None and rhs_meta is None and rhs_node.meta["val"].numel() == 1:
        copy_chunking_meta(div_node, lhs_meta)
        return True

    return False


@register_propagate_rule([
    aten.sum.default,
])
def propagate_sum_to_scalar(sum_node):
    input_node = sum_node.args[0]
    input_meta = get_chunking_meta(input_node)
    assert input_meta

    # Input is not chunked
    if input_meta.chunk_dim is None:
        return False

    out_meta = ChunkingMeta(**dataclasses.asdict(input_meta))
    out_meta.need_sum = True
    out_meta.chunk_dim = None

    set_chunking_meta(sum_node, meta=out_meta)
    return True


@register_propagate_rule([
    aten.amax.default,
    aten.sum.dim_IntList,
])
def propagate_reduce_non_chunk_dim(reduce_node):
    """
    A reduction that reduces across non-chunked dimension
    """
    arg_node, reduce_dims = reduce_node.args[0: 2]
    arg_meta = get_chunking_meta(arg_node)
    if arg_meta is None or arg_meta.chunk_dim in reduce_dims:
        return False
    
    copy_chunking_meta(reduce_node, arg_node)
    return True
